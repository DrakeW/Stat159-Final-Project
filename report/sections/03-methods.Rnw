\section{Methods}

\begin{enumerate}[]
  \item \textbf{Ordinary Least Square methods} \\
  In statistics, OLS is a method for estimating the coefficients in a linear regression model, with the goal of minimizing the sum of the squares of the differences between the observed responses in the given dataset and those predicted by the linear model. (i.e.  $RSS = \sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p(\beta_{j}x_{ij}))^2$, where $\beta_0 ... \beta_p$ are the coefficients estimate)
  
  \begin{enumerate}[1.]
      \item \textbf{Ordinary Least Squares (OLS)} \\
      This is a common linear regression model, and we use it as a benchmark to evaluate the performance of the other 4 regression models.
  \end{enumerate} 
  
  \item \textbf{Shrinkage methods} \\
  Shrinkage methods involves fitting a model with all P predictor. However, the estimated coefficients are shrunken towards 0 relative to the least squares estimates. Shrinkage has the effect of reducing variance. Since after the process of shrinkage, some of the coefficients might be exactly 0, it also performs variable selection.
  
  \begin{enumerate}[1.]
    \item \textbf{Ridge Regression (Ridge)} \\
    Ridge regression is very similar to least square, except that the coefficients are estimated by minimizing $RSS + \lambda \sum \beta_j^2$. This equation involves the minimization of two criteria, the first one is the same as OLS, ridge regersssion seeks to fit the data well by minimizing RSS. The second part of this equation, $\lambda \sum \beta_j^2$, is called the \textbf{shrinkage penalty}. And the shrinkage penalty gets smaller as coefficients get closer to 0. $\lambda$ here is called the tuning parameter, and $\lambda \geq 0$. When $\lambda = 0$, Ridge regression is the same as OLS. As $\lambda$ grows larger, the impact of shrinkage penalty grows bigger as well. The use of tuning parameter $\lambda$ is also where Ridge regression outperforms OLS, and the mechanism's name is called \textbf{bias variance trade off}. As $\lambda$ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias
    
    \item \textbf{Lasso Regression (Lasso)}
    Lasso regression is an alternative to Ridge and it overcomes oen of Ridge's disadvantage, that even though it uses all P predictors and even though it moves all coefficients close to 0, it won't set any of them to be exactly 0 unless $\lambda$ is infinity and this disadvantage creates obstacles on model interpretation when P gets really large. Lasso regression minimizes $RSS + \lambda \sum |{\beta_j}|$. And it is the 2nd term that forces some of the coefficients estimates to be exactly 0.
  \end{enumerate}
  
  \item \textbf{Dimension Reduction methods} \\
  Dimension reduction works by \textbf{projecting} the P predictors onto a M-dimensional space, where M $\textless$ P. This is achieved by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares.
  
  \begin{enumerate}[1.]
    \item \textbf{Principal Components Regression (PCR)} \\
    The \textbf{first principal component} direction of the data is that along which the observation vary the \textbf{most}. The PCR approach involves constructing the first M principal components, $Z_1, Z_2,...,Z_M$, and then using those principle components as predictors in a linear regression model that uses least squares method. This approach reduces dimension because of the fact that usually a smaller amount ($M < P$) of principal components are sufficient to explain most of the variability in the data set, as well as the relationship with the response. PCR is an \textbf{unsupervised} approach because in the process of finding the first M principal components that best describes the P predictors, the response Y is not used to help determine the principal component direction.
    
    \item \textbf{Partial Least Square Regression (PLSR)} \\
    PLSR is a \textbf{supervised} alternative to PCR. It first identifies a new set of features $Z_1, Z_2,...,Z_M$ that are linear combinations of the original features, and then fits a linear model using least squares method with those M features. The difference is PLSR identifies $Z_1, Z_2,...Z_M$ in a supervised way, which means it makes use of the response Y while identifying new features, therefore the new features not only approximate the old feature well enough, they are also related the response.
  \end{enumerate} 
\end{enumerate}