---
title: "Final Project Presentation"
author: "Junyu Wang, Nichole Rethmeier, Jie Sun, Stephen (Mingtao) Fang"
date: "December 5, 2016"
output: ioslides_presentation
---

## Introduciton
In this project we address a group of administrators trying to make their school more competitive, both in diversity and graduation rates. 


We use the College Scorecard data to apply a predictive model in order to determine factors that may influence greater diversity and graduation rates for schools within California.

## Data

The data used in this project was from the College Scorecard. The College Scorecard, introduced during the Obama administration, is maintained by the U.S. Department of Education. 

This program provides data around all colleges in the United States as a means of helping students compare different institutions to choose the most suitable for themself. 


## Methods

- Ordinary Least Square
- Ridge Regression
- Lasso Regression
- Principal Components Regression
- Partial Least Sqaure Regression

## Shrinkage Methods

- Shrinkage methods involve fitting a model using all predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance. 

- Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection. 

## Ridge Regression

The ridge regression coefficient estimates are the values that minimize:
$$\sum\limits^n_{i=1}\bigg(y_i - \beta_0 - \sum\limits^p_{j=1}\beta_j x_{ij}\bigg)^2 + \lambda \sum\limits^p_{j=1} \beta_j^2 = \text{RSS} + \lambda \sum\limits^p_{j=1} \beta^2_j$$
Ridge regression’s advantage over least squares is rooted in the bias-variance trade-off. As $\lambda$ (a tuning parameter) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. 


## Lasso Regression

The lasso coefficients minimize the quality:
$$\sum\limits^n_{i=1}\bigg(y_i - \beta_0 - \sum\limits^p_{j=1}\beta_j x_{ij}\bigg)^2 + \lambda \sum\limits^p_{j=1} |\beta_j| = \text{RSS} + \lambda \sum\limits^p_{j=1} |\beta_j|$$
Lasso overcomes the disadvantage of ridge regression by using $l_1$ penalty, which has the effect of forcing some of the coefficient estimates to be exactly equal to zero when $\lambda$ is sufficiently large. 
  			
## Dimension Reduction

- Dimension reduction involves projecting the $p$ predictors into a $M$-dimensional subspace, where $M$ < $p$. This is achieved by computing $M$ different linear combinations, or projections, of the variables. 
- These $M$ projections are used as predictors to fit a linear regression model by least squares. In this way, a low-dimensional set of features can be derived from a large set of variables.

## Principal Components Regression			

- Using the principal components analysis (PCA), this regression approach involves constructing the first $M$ principal components, $Z_1,...,Z_M$, and then using these components as the predictors in a linear regression model that is fit using least squares. 

- The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. 


## Partial Least Squares Regression

- The partial least squares (PLS) is a supervised alternative to PCR. Like PCR, PLS first identifies a new set of features $Z_1,...,Z_M$ that are linear combinations of the original features, and then fits a linear model via least squares using these $M$ new features. 

- Unlike PCR, PLS identifies these new features in a supervised way — that is, it makes use of the response $Y$ in order to identify new features that not only approximate the old features well, but also that are related to the response. Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors. 

## Steps of Analysis

- Data Cleaning
- Splitting Training Set and Test set
- Regression Modeling

## Data Cleaning

- The original data set has more than 1700 columns but we certainly don't need to use all of them. Since we are measuring competitiveness in terms of diversity and completion rate, we picked related columns like `UGDS_MEN`, `UGDS_WOMEN`, `UGDS_WHITE`, `C100\_4` etc.

- The columns of data we read in are mostly of type character and factor, so we need to convert them to numeric values for further computation and regression.

- There are a lot of missing values in columns like `C100\_4` (completion rate) which are essential to our regression, so what we did is we first removed columns that has more than 75% NAs in it, and then we delete rows with NAs or with non-numeric data

## Diversity Formula

We need a way to numerically measure diversity of each school, so we came up with our own diversity score formula. In order to define diversity in terms of `Gender, Race, Marital Status, and ratio of First Generation Student`, we calculated the {chi-square} ($x_c^2 = \sum\frac{(O_i - E_i)^2}{E_i}$) of columns belonged those those 4 categories separately as their own diversity scores, then take the average as the final diversity score.

## Training Set and Test Set
- our training set is 300 (out of 394) rows randomly selected from our cleaned data set for diversity regressions, and 80 rows (out of 105) from our data set for completion rate regressions, using the $sample()$ function and the rest of rows are the test set. We also used the $set.seed()$ function for reproducibility.

## Model Building

- For each of our regressions we fit the model to our training set after performing 10-fold-cross-validation and resampling the data.

- We use our test set to compute the Mean Square Error to compare all of our best models. Finally to finish off our regressions we used the full set of data to get our actual coefficients for the models. 

- The full data set was used in combination with our regression functions to compute summary statistics such as $R^{2}$, F-Statistic, and more.


## Results

Summary of all regression models' coefficients

```{r results='asis', echo=FALSE}
library(xtable)
library(Matrix)
load(paste0("../data/regressions/ols-diversity-model.RData"))
load(paste0("../data/regressions/ols-grad-rate-model.RData"))
model_name = c("ridge", "lasso", "pcr", "plsr")
for (i in model_name) {
  load(paste0("../data/regressions/", i, "-diversity-models.RData"))
  load(paste0("../data/regressions/", i, "-grad-rate-models.RData"))
}

ols_div = as.vector(ols.coef[-1])
ridge_div = as.vector(ridge_official_coef[-1])
lasso_div = as.vector(lasso_official_coef[-1])
pcr_div = as.vector(pcr_official_coef)
plsr_div = as.vector(plsr_official_coef)
div_df = data.frame(ols_div, ridge_div, lasso_div, pcr_div, plsr_div)
rnames = names(ols.mod$coefficients[-1])
row.names(div_df) = rnames
colnames(div_df) = c("OLS", "Ridge", "Lasso", "PCR", "PLSR")

div_table = xtable(div_df, digits=c(0,4,4,4,4,4), caption = "Regression Coefficients for All Diversity Models")
print(div_table, comment=FALSE, type="html")
```

## Results - Diversity Regression

```{r results='asis', echo=FALSE}
ols_grad = as.vector(ols.coef.grad[-1])
ridge_grad = as.vector(ridge_official_coef.grad[-1])
lasso_grad = as.vector(lasso_official_coef.grad[-1])
pcr_grad = as.vector(pcr_official_coef.grad)
plsr_grad = as.vector(plsr_official_coef.grad)
grad_df = data.frame(ols_grad, ridge_grad, lasso_grad, pcr_grad, plsr_grad)
rnames = names(ols.mod.grad$coefficients[-1])
row.names(grad_df) = rnames
colnames(grad_df) = c("OLS", "Ridge", "Lasso", "PCR", "PLSR")

grad_table = xtable(grad_df, digits=c(0,3,3,4,3,3), caption = "Regression Coefficients for All Graduation Rate Models")
print(grad_table, comment=FALSE, type="html")
```

## Results - Graduation Regression

```{r results='asis', echo=FALSE}
div_mse_df = data.frame(row.names=c("Test MSE"), 
                    Ridge=ridge_test_mse, Lasso=lasso_test_mse, 
                    PCR=pcr_test_mse, PLSR=plsr_test_mse)
div_mse_table = xtable(div_mse_df, digits=3, caption = "Test MSE Values for All Diversity Models")
print(div_mse_table, comment=FALSE, type="html")

grad_mse_df = data.frame(row.names=c("Test MSE"), 
                    Ridge=ridge_test_mse.grad, Lasso=lasso_test_mse.grad, 
                    PCR=pcr_test_mse.grad, PLSR=plsr_test_mse.grad)
grad_mse_table = xtable(grad_mse_df, digits=3, caption = "Test MSE Values for All Graduation Rate Models")
print(grad_mse_table, comment=FALSE, type="html")
```

## Conclusion - EDA

From our exploratory data analysis, we were able to gain some insights into our intial data. 

- For example, the variables that impacted our diversity score the most were those used to compute our racial diversity score, `UGDS_WHITE`, `UGDS_BLACK`, `UGDS_HISP`, `UGDS_ASIAN`, `UGDS_AIAN`, and `UGDS_NHPI`. This is to be expected as the disparities between racial diversity at the university were larger than those of gender. 

- Another interesting correlation that was discovered was between \texttt{PCTFLOAN} and minority groups such as `UGDS_BLACK` and  `UGDS_NHPI`. Looking at the Descriptive Statistics table in `data/eda-output.txt` we can see that on average the proportion of Black students at universities in California was less than 10\% and the proportion of Native Hawaiian and Pacific Islanders at universities in California was less than 1\%. 

## Conclusion - Regression Analyses

As our clients wished to compare themselves to other schools in relation to diversity and graduation rates, we ran two separate regression analyses upon the diversity score and graduation rates. 

## Conclusion - Diversity Regression

- We can see that the `ICLEVEL` variable had the largest impact upon our dependent variable `DIV_SCORE`. Referring back to the data section, `ICLEVEL` represented whether the institution was a 4-year, 2-year, or less that 2-year institution. As these factors are often indicators of other things about a university, it makes sense that they should play an important role in our regression on diversity.

- Therefore, one recommendation for making your institution more competitive from a diversity stand point would be to offer 4-year programs. This will attract more students to the institution resulting in more diversity. We extend our recommendation to not just offering 4-year programs, but offering higher degrees as well, which goes hand-in-hand, as we saw that `HIGHDEG` was another predictor that had influenced diversity. This is similar in that students will be more attracted to attaining a higher degree, such as a Bachelors, as opposed to an Associates degree. 

## Conclusion - Graduation Regression

- One of the most interesting conclusions from this analysis is the correlation between `PCTFLOAN` and graduation rates. `PCTFLOAN` represents the share of student who receive federal loans. From this, we can infer that the greater the share of students that accepted federal loans to help pay for college, the more likely the student was to graduate. 

- To increase the institutions graduation rates, we recommend that the group of administrators offer education programs for students about: 
  
  - Types of federal loans offered 
  - How to determine if you need federal loans or not
  - Plans for paying back federal loans after graduating
